{"cells":[{"cell_type":"markdown","source":["**Create pseudo labels during the semi-supervised phase.**"],"metadata":{"id":"aJFngX8ACDYi"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24198,"status":"ok","timestamp":1702797558673,"user":{"displayName":"mrinal kanti Dhar","userId":"05916082757615620540"},"user_tz":360},"id":"NtZrR6Lpamzt","outputId":"81ad1f4d-62d6-4038-a19e-3f80fccb07cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKYTa1YNahpR"},"outputs":[],"source":["import sys\n","\n","sys.path.append('/content/drive/MyDrive/Wound_tissue_segmentation')\n","sys.path.append('/content/drive/MyDrive/Wound_tissue_segmentation/utils')\n","sys.path.append('/content/drive/MyDrive/Wound_tissue_segmentation/wound_lib')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wWvUuJ7xZ17E"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset as BaseDataset\n","import albumentations as albu\n","import cv2\n","import numpy as np\n","import segmentation_models_pytorch as smp\n","from segmentation_models_pytorch.utils import metrics, losses, base\n","import random\n","import matplotlib.pyplot as plt\n","import os\n","from copy import deepcopy\n","from datetime import datetime\n","import torch.nn.functional as F\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"u1TTozpn-D8o"},"source":["## Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nx_iN5gGarSj"},"outputs":[],"source":["class Dataset(BaseDataset):\n","    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n","\n","    Args:\n","        images_dir (str): path to images folder\n","        masks_dir (str): path to segmentation masks folder\n","        augmentation (albumentations.Compose): data transfromation pipeline\n","            (e.g. flip, scale, etc.)\n","        preprocessing (albumentations.Compose): data preprocessing\n","            (e.g. noralization, shape manipulation, etc.)\n","\n","    \"\"\"\n","\n","    def __init__(\n","            self,\n","            list_IDs,\n","            images_dir,\n","            preprocessing=None,\n","            resize=(False, (256, 256)), # To resize, the first value has to be True\n","            n_classes:int=4,\n","    ):\n","        self.ids = list_IDs\n","        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n","\n","        self.preprocessing = preprocessing\n","        self.resize = resize\n","        self.n_classes = n_classes\n","\n","    def __getitem__(self, i):\n","\n","        # read data\n","        image = cv2.imread(self.images_fps[i])\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        if self.resize[0]:\n","            image = cv2.resize(image, self.resize[1], interpolation=cv2.INTER_NEAREST)\n","\n","        # apply preprocessing\n","        if self.preprocessing:\n","            sample = self.preprocessing(image=image)\n","            image = sample['image']\n","\n","        return image\n","\n","    def __len__(self):\n","        return len(self.ids)"]},{"cell_type":"markdown","metadata":{"id":"yB8RKETkALtF"},"source":["## Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T-ltFwaGAK3c"},"outputs":[],"source":["def get_training_augmentation():\n","    train_transform = [\n","\n","        albu.OneOf(\n","            [\n","                albu.HorizontalFlip(p=0.5),\n","                albu.VerticalFlip(p=0.5),\n","            ],\n","            p=0.8,\n","        ),\n","\n","        albu.OneOf(\n","            [\n","                albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0, p=0.1, border_mode=0), # scale only\n","                albu.ShiftScaleRotate(scale_limit=0, rotate_limit=30, shift_limit=0, p=0.1, border_mode=0), # rotate only\n","                albu.ShiftScaleRotate(scale_limit=0, rotate_limit=0, shift_limit=0.1, p=0.6, border_mode=0), # shift only\n","                albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=30, shift_limit=0.1, p=0.2, border_mode=0), # affine transform\n","            ],\n","            p=0.9,\n","        ),\n","\n","        albu.OneOf(\n","            [\n","                albu.Perspective(p=0.2),\n","                albu.GaussNoise(p=0.2),\n","                albu.Sharpen(p=0.2),\n","                albu.Blur(blur_limit=3, p=0.2),\n","                albu.MotionBlur(blur_limit=3, p=0.2),\n","            ],\n","            p=0.5,\n","        ),\n","\n","        albu.OneOf(\n","            [\n","                albu.CLAHE(p=0.25),\n","                albu.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.25),\n","                albu.RandomGamma(p=0.25),\n","                albu.HueSaturationValue(p=0.25),\n","            ],\n","            p=0.3,\n","        ),\n","\n","    ]\n","\n","    return albu.Compose(train_transform, p=0.9) # 90% augmentation probability\n","\n","\n","def get_validation_augmentation():\n","    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n","    test_transform = [\n","        # albu.PadIfNeeded(512, 512)\n","    ]\n","    return albu.Compose(test_transform)\n","\n","\n","def to_tensor(x, **kwargs):\n","    return x.transpose(2, 0, 1).astype('float32')\n","\n","\n","def get_preprocessing(preprocessing_fn):\n","    \"\"\"Construct preprocessing transform\n","\n","    Args:\n","        preprocessing_fn (callbale): data normalization function\n","            (can be specific for each pretrained neural network)\n","    Return:\n","        transform: albumentations.Compose\n","\n","    \"\"\"\n","\n","    _transform = [\n","        albu.Lambda(image=preprocessing_fn),\n","        albu.Lambda(image=to_tensor, mask=to_tensor),\n","    ]\n","    return albu.Compose(_transform)"]},{"cell_type":"markdown","metadata":{"id":"Z9xxDHfp_yFO"},"source":["## Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Yq5o1vzBYxN0"},"outputs":[],"source":["# Parameters\n","BASE_MODEL = 'MiT+pscse'\n","ENCODER = 'mit_b3'\n","ENCODER_WEIGHTS = 'imagenet'\n","BATCH_SIZE = 16\n","n_classes = 4\n","ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","LR = 0.0001 # learning rate\n","EPOCHS = 500\n","WEIGHT_DECAY = 1e-5\n","SAVE_WEIGHTS_ONLY = True\n","RESIZE = (False, (256,256)) # if resize needed\n","TO_CATEGORICAL = True\n","SAVE_BEST_MODEL = True\n","SAVE_LAST_MODEL = False\n","\n","PERIOD = 10 # periodically save checkpoints\n","RAW_PREDICTION = False # if true, then stores raw predictions (i.e. before applying threshold)\n","RETRAIN = False\n","\n","# For early stopping\n","EARLY_STOP = True # True to activate early stopping\n","PATIENCE = 50 # for early stopping\n","\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context"]},{"cell_type":"markdown","metadata":{"id":"Mp9ohG-nyZzg"},"source":["\n","## Load model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ja17lcFcb7wB"},"outputs":[],"source":["save_dir_pred_root = '/content/drive/MyDrive/Wound_tissue_segmentation/predictions'\n","os.makedirs(save_dir_pred_root, exist_ok = True)\n","\n","# create segmentation model with pretrained encoder\n","model = smp.Unet(\n","    encoder_name=ENCODER,\n","    encoder_weights=ENCODER_WEIGHTS,\n","    # aux_params=aux_params,\n","    classes=n_classes,\n","    activation=ACTIVATION,\n","    decoder_attention_type='pscse',\n",")\n","\n","preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n","\n","model.to(DEVICE)\n","\n","# Optimizer\n","optimizer = torch.optim.Adam([\n","    dict(params=model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY),\n","])\n","\n","# Learning rate scheduler\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n","                              factor=0.1,\n","                              mode='min',\n","                              patience=10,\n","                              min_lr=0.00001,\n","                              verbose=True,\n","                              )\n","\n","# Model\n","model_name = 'MiT+pscse_padded_mit_b3_2023-11-24_11-58-23_selfSupervised'       # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< pay attention\n","print(model_name)\n","\n","# Checkpoint directory\n","checkpoint_loc = '/content/drive/MyDrive/Wound_tissue_segmentation/checkpoints/' + model_name\n","\n","# =================================== Inference ================================\n","# Load model====================================================================\n","checkpoint = torch.load(os.path.join(checkpoint_loc, 'best_model.pth'))\n","model.load_state_dict(checkpoint['state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"v0UF_1PHQVM0"},"outputs":[],"source":["\"\"\"\n","IMPORTANT:\n","Uncomment only for this run. Here, creating duplicates of unsupervised images. Images will be\n","deleted after each run.\n","\n","In next runs, DO NOT run this cell.\n","\"\"\"\n","\n","import shutil\n","\n","copy_src = '/content/drive/MyDrive/Wound_tissue_segmentation/Dataset/FUWound_mmseg_all_in_one_cropped_padded/unsupervised/images'\n","\n","copy_dst = '/content/drive/MyDrive/Wound_tissue_segmentation/Dataset/FUWound_mmseg_all_in_one_cropped_padded/unsupervised/images_del_'\n","\n","os.makedirs(copy_dst, exist_ok=True)\n","\n","names = os.listdir(copy_src)\n","\n","for name in names:\n","  shutil.copy(os.path.join(copy_src, name), os.path.join(copy_dst, name))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IALquySrcE7x"},"outputs":[],"source":["x_test_dir = '/content/drive/MyDrive/Wound_tissue_segmentation/Dataset/FUWound_mmseg_all_in_one_cropped_padded/unsupervised/images_del_'\n","\n","print('No. of test images: ', len(os.listdir(x_test_dir)))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kaGAQQ1VSxcy"},"outputs":[],"source":["# Delete images from previous phase\n","# Replace existing images using phase-1 images\n","txt_phase1 = 'MiT+pscse_padded_mit_b3_2023-11-24_11-58-23_selfSupervised_unsup_train.txt'\n","dir_txt_phase1 = '/content/drive/MyDrive/Wound_tissue_segmentation/texts/' + txt_phase1 # full directory\n","dir_annotations_phase1 = '/content/drive/MyDrive/Wound_tissue_segmentation/predictions/' + 'MiT+pscse_padded_mit_b3_2023-11-24_11-58-23_selfSupervised'\n","\n","# First, read the text file\n","with open(dir_txt_phase1, \"r\") as f:\n","  phase_names = f.readlines()\n","\n","phase_names = [name.split('\\n')[0] for name in phase_names] # remove \\n (newline)\n","\n","# First, delete existing annotations\n","for name in phase_names:\n","  if os.path.exists(os.path.join(x_test_dir, name)): os.remove(os.path.join(x_test_dir, name))\n","\n","list_IDs_test = os.listdir(x_test_dir)\n","\n","print('No. of test images: ', len(list_IDs_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MwRA6u_4ZvyE"},"outputs":[],"source":["# Test dataloader ==============================================================\n","test_dataset = Dataset(\n","    list_IDs_test,\n","    x_test_dir,\n","    preprocessing=get_preprocessing(preprocessing_fn),\n","    resize=(RESIZE),\n","    n_classes=n_classes,\n",")\n","\n","test_dataloader = DataLoader(test_dataset,\n","                            batch_size=1,\n","                            shuffle=False,\n","                            num_workers=6)\n","\n","# Prediction ===================================================================\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from sklearn.metrics import confusion_matrix\n","import scipy.io as sio\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","save_pred = True\n","threshold = 0.5\n","ep = 1e-6\n","raw_pred = []\n","\n","HARD_LINE = True\n","\n","# Save directory\n","save_dir_pred = '/content/drive/MyDrive/Wound_tissue_segmentation/predictions/' + model_name + '_phase2'\n","if not os.path.exists(save_dir_pred): os.makedirs(save_dir_pred)\n","\n","save_dir_pred_palette = '/content/drive/MyDrive/Wound_tissue_segmentation/predictions_palettte/' + model_name + '_phase2'\n","if not os.path.exists(save_dir_pred_palette): os.makedirs(save_dir_pred_palette)\n","\n","iter_test_dataloader = iter(test_dataloader)\n","\n","palette = [[128, 128, 128], [255, 0, 0], [0, 255, 0], [0, 0, 255]]\n","\n","for enu, i in enumerate(range(len(list_IDs_test))):\n","\n","    name = os.path.splitext(list_IDs_test[i])[0] # remove extension\n","\n","    print('Processing:', enu, name)\n","\n","    # Image-wise mean of metrics\n","    i_mp, i_mr, i_mdice, i_miou = [], [], [], []\n","\n","    image = next(iter_test_dataloader) # get image and mask as Tensors\n","\n","    # Note: Image shape: torch.Size([1, 3, 512, 512]) and mask shape: torch.Size([1, 1, 512, 512])\n","\n","    pr_mask = model.predict(image.to(DEVICE)) # Move image tensor to gpu\n","\n","    # Convert from onehot\n","    # gt_mask = torch.argmax(gt_mask_, dim=1)\n","    if TO_CATEGORICAL:\n","        pr_mask = torch.argmax(pr_mask, dim=1)\n","\n","    # pr_mask = torch.argmax(pr_mask, dim=1)\n","\n","    # Move to CPU and convert to numpy\n","    pred = pr_mask.squeeze().cpu().numpy()\n","\n","    # Save raw prediction\n","    if RAW_PREDICTION: raw_pred.append(pred)\n","\n","    # Modify prediction based on threshold\n","    # pred = (pred >= threshold) * 1\n","\n","    # Save prediction as png\n","    if save_pred:\n","        \"Uncomment for non-palette\"\n","        cv2.imwrite(os.path.join(save_dir_pred, list_IDs_test[i]), np.squeeze(pred).astype(np.uint8))\n","\n","        \"Uncomment for palette\"\n","        # Palette prediction\n","        pal_pred = np.squeeze(pred).astype(np.uint8)\n","        pal_pred = Image.fromarray(pal_pred)\n","        pal_pred = pal_pred.convert(\"P\")\n","        pal_pred.putpalette(np.array(palette, dtype=np.uint8))\n","\n","        # Store\n","        pal_pred.save(os.path.join(save_dir_pred_palette, list_IDs_test[i]))"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}